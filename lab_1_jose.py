# -*- coding: utf-8 -*-
"""Lab_1_Jose

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UBHTlaMQwPsd4GHGlC3VUpWxaMFurSiq
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

"""# 1. Warm-up (4 points)

## 1.1 Creating Matrices (0.25 points)

Create 4 matrices
- A - a "checkerboard" matrix of zeros and ones, size 6x3, with the top-left value (A[0][0]) equal to 1,
- В - a matrix of numbers from 1 to 24, arranged in a snake-like pattern, size 6x4,
- C - a matrix of random integers from 2 to 10 (inclusive), size 4x3,
- D - a matrix of zeros with ones on the main diagonal, size 4x4.

**Create a “patchwork” matrix S from these matrices**

A В

С D

using **only standard** numpy operations. Do not use Comprehensions.

Then, append matrix **F** of size 10x2 filled with zeros to the resulting matrix **S** to create matrix **G**:

S F

Note: When we say a matrix has a size of x by y, x is the number of rows, and y is the number of columns.
"""

A = np.array([[0, 1, 0],
              [1, 0, 1],
              [0, 1, 0],
              [0, 1, 0],
              [0, 1, 0],
              [0, 1, 0]])
A
B = np.array([[1,2,3,4],
              [8,7,6,5],
              [9,10,11,12],
              [16,15,14,13],
              [17,18,19,20],
              [24,23,22,21]])
B
C = np.random.randint(2, 10, (4,3) )
C
D =  np.array([[1,0,0,0],
             [0,1,0,0],
             [0,0,1,0],
             [0,0,0,1]])
D
print(A)
print(B)
print(C)
print(D)



# YOUR CODE HERE

"""## 1.2 Finding the Nearest Neighbor (0.25 points)

Implement a function that takes a matrix **X** and a number **a** and returns the element in the matrix closest to the given number.
   
For example, for **X = np.arange(0, 10).reshape((2, 5))** and **a = 3.6**, the answer will be 4. You can only use basic numpy functions, **do not use loops**.
"""

def find_nearest_neighbour(X, a):
  difference =np.abs(X - a)
  result = np.argmin(difference)
  return result
X=np.arange(0,10).reshape((2,5))
a = 3.6
print(find_nearest_neighbour(X,a))

"""## 1.3 Very Strange Neural Network (0.25 points)

Implement a strange neural network. The network should:

- Square matrix **A** (the weight matrix) of size N x N.
- In the first transformation, multiply a vector **X** of length N (feature vector) by the weight matrix **A^2** (the output will be a new vector).
- In the second transformation, multiply the resulting vector by vector **b** (weight vector) of size N to produce a scalar value.

Assume that all elements in matrices and vectors are floating-point numbers.
"""

def very_strange_neural_network(A, b, X):
    squared_matrix =  np.dot(A,A)
    first_transformation = np.dot(squared_matrix,X)
    second_transformation = np.dot(first_transformation,b)
    return second_transformation

A = np.array([[2,3,4,5],
              [4,12,9,8],
              [10,1,3,8],
              [11,4,5,3]])
b = np.array([3,6,7,8])
X =np.array([3,1,2,5])
print(very_strange_neural_network(A,b,X))

"""## 1.4 The Jungle Calls! (0.25 points)

You are given a matrix **M**, a map of an impassable jungle terrain created by Lara Croft. Each cell in the map is an integer representing the height above sea level (if positive) in meters or the sea depth (if negative) in meters in a one-meter-by-one-meter area of the map. If the number is 0, it represents land - a shoreline.

You need to calculate:
- The total area of cells in the sea where the depth is greater than 5 (in m^2).
- The total volume of water on the map (in m^3).
- The maximum height above sea level on this map (in m).
"""

def find_deep_sea_area(M):
    deep_sea_area = M < -5
    return np.sum(deep_sea_area)

def find_water_volume(M):
    water = M < 0
    return np.sum(abs(M[water])).sum()

def find_max_height(M):
    return np.max(M[M>0])

# You can create your own example.
M = np.array([
    [-7, -3, -1, 0],
    [-4, -3, 1, 19],
    [-2, 0, 4, 25],
    [-1, 3, 6, 9]
])

# simple check for the example above
assert np.isclose(find_deep_sea_area(M), 1)
assert np.isclose(find_water_volume(M), 21)
assert np.isclose(find_max_height(M), 25)

print("Total sea area on the map -", find_deep_sea_area(M), "м^2")
print("Total water volume on the map -", find_water_volume(M), "м^3")
print("Maximum elevation on the map -", find_max_height(M), "м")

"""## 1.5 Treasure Islands (0.25 points)


The function takes an array **a** of zeros and ones as input. Count the number of consecutive blocks of ones (islands) in the array. Only basic numpy functions are allowed, **no loops**.

Hint: check what `np.diff` does.
"""

def count_all_islands(a):
    difference = np.diff(a)
    counter = np.sum(difference == 1)

    return counter

a = np.array([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1])

assert count_all_islands(a) == 4
print(count_all_islands(a))

"""## 1.6 Masquerade (0.25 points)

The input is a 2D matrix **X** filled with floating-point numbers and a floating-point number **a**. Replace all cells in the matrix greater than **a** with the average value of the elements in **X**.

**You must** use basic numpy functions, **no loops**.
"""

def swap_mask_for_average(X, a):
    average_value = X.mean()
    X[X > a] = average_value
    return X

# You can create your own example.
M = np.array([
    [-7, -3, -1, 0],
    [-4, -3, 1, 19],
    [-2, 0, 4, 25],
    [-1, 3, 6, 9]
])
a = 5

# simple check for the example above
assert np.allclose(swap_mask_for_average(M, a),
                   np.array([
                       [-7, -3, -1, 0],
                       [-4, -3, 1, 2],
                       [-2, 0, 4, 2],
                       [-1, 3, 2, 2]
                   ]))

swap_mask_for_average(M, a)

"""## 1.7 Hot on the Trails (0.25 points)

The input is a square matrix **M**. Calculate the difference between the sum along the main diagonal and the secondary diagonal.

Only basic numpy functions are allowed, **no loops**.

Hint: look up `np.trace`.
"""

def count_trace_diff(M):
    first_trace = M.trace()
    inverse = np.fliplr(M)
    second_trace =inverse.trace()
    answer = first_trace - second_trace
    return answer

# You can create your own example.
M = np.array([
    [-7, -3, -1, 0],
    [-4, -3, 1, 19],
    [-2, 0, 4, 25],
    [-1, 3, 6, 9]
])

# simple check for the example above
assert np.allclose(count_trace_diff(M), 3)

count_trace_diff(M)

"""## 1.8 King of the Hill (0.25 points)

The input is a vector a of size N. Using addition, concatenation, and broadcasting, create a symmetric matrix of size 2N x 2N with the maximum value in the center and decreasing values toward the edges.

Example: a = (0, 1, 2)

Result:

0 1 2 2 1 0 \\
1 2 3 3 2 1 \\
2 3 4 4 3 2 \\
2 3 4 4 3 2 \\
1 2 3 3 2 1 \\
0 1 2 2 1 0 \\
"""

def create_mountain(a):
    base = np.concatenate([a, a[::-1]])
    return base[:, None] + base[None, :]

# You can create your own example.
a = np.array([0, 1, 2, 3, 4])

create_mountain(a)

"""## 1.9 Monochrome Photograph 9x12 (0.5 points)

The input is a 2D matrix **P** of size N x M filled with numbers from 0 to 255, representing a grayscale photograph, and a natural number **C**. You need to produce a matrix of size (N - C + 1) x (M - C + 1), where each cell is the average value of the corresponding sub-matrix of size **C x C**. Essentially, this will apply a simple blur effect (slightly reducing its size).
"""

def custom_blur(P, C):
    N, M = P.shape
    shape = (N - C + 1, M - C + 1, C, C)
    strides = P.strides * 2
    sub_matrices = np.lib.stride_tricks.as_strided(P, shape, strides)
    return sub_matrices.mean(axis=(2, 3))

# You can create your own example.
P = np.arange(0, 12).reshape((3, 4))
kernel = 2

# simple check for the example above
assert np.allclose(custom_blur(P, kernel),
                   np.array([[2.5, 3.5, 4.5], [6.5, 7.5, 8.5]]))
custom_blur(P, 2 )

"""## 1.10 Validation Function (0.75 points)

The input to the function is an arbitrary number (>2) of tuples representing the shapes of different matrices. The function should return True if the matrices can be sequentially added together (possibly using broadcasting), and False otherwise.
"""

def check_successful_broadcast(*matrices):
    result_shape = matrices[0]
    for shape in matrices[1:]:
        new_shape = ()
        current_shape = (1,) * (len(shape) - len(result_shape)) + result_shape
        candidate_shape = (1,) * (len(result_shape) - len(shape)) + shape
        for a, b in zip(current_shape, candidate_shape):
            if a == b or a == 1 or b == 1:
                new_shape += (max(a, b),)
            else:
                return False
        result_shape = new_shape
    return True

assert check_successful_broadcast((5, 6, 7), (6, 7), (1, 7))
# You can test with your examples

"""## 1.11 Pairwise Distances (0.75 points)

The input is matrices A of size m x k and B of size n x k. Create a matrix of size m x n containing pairwise Euclidean distances.

Only use basic functions, do not use loops or third-party libraries. Broadcasting will probably be useful. The solution must be **in one line**, following all code style rules.
"""

def pairwise_distances(A, B):
    return ((A[:, None, :] - B[None, :, :]) ** 2).sum(axis=2) ** 0.5

A = np.array([[1, 2], [3, 4], [5, 6]])
B = np.array([[7, 8], [9, 10]])

pairwise_distances(A, B)

"""Explain the logic behind this one-liner. What exactly is happening?
Basically each row of A is now reshaped to an slice of size 1Xk helping us to compare with B
similarly each row of B is reshaped so it is also 1Xk
then substract every row of B from every row of A
then square the differences
sum(axis) sums the squared differences
finally 0.5 takes square root so we get euclidean distance so the resulting matrix is the one with the euclidean distances of the points

<font color='red'> YOUR ANSWER HERE </font>

# 2. Data Experiment Processing (3 points)

Ladies and gentlemen, now we're going to learn to use libraries for data analysis in real-life scenarios!

**The reason behind this section is simple**: many students in the Faculty of Physics and Mathematics still rely on Excel, calculators, or pen and paper even in their second or third semesters. Our goal is to introduce another method for conducting laboratory work with a much lower entry barrier than Excel itself. We hope this motivates some to explore these handy libraries further.

*Data sponsor for this section - blacksamorez. Without them, five happy semesters of labs would have been far less joyful...*
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""##  2.1. Problem Statement and Data

Let's assume we have a gyroscope with a weight attached to its axis on an arm (see the diagram for a quick understanding, and you can find more details in the [laboratory manual](https://lib.mipt.ru/book/267519/), volume 1, p.160). Due to the weight, the gyroscope begins to slowly [precess](https://en.wikipedia.org/wiki/Precession), i.e., it rotates around the vertical axis with a relatively constant frequency.

We'll work through part of this lab, primarily focusing on data processing and plotting graphs.

<center><img src='https://drive.google.com/uc?export=view&id=1KfYQ0hKYRDhi5uk7C8lNffZBNy8NF7nu' width=600>

Diagram of the gyroscope with the attached weight G and arm C</center>

First, let's examine the data someone has kindly gathered for us. Create a dataframe from [this file](https://drive.google.com/file/d/1SbLR6R16obqLewUTnX1CAAjQTrVXh2Vq/view?usp=sharing)
"""

# YOUR CODE HERE
data = pd.read_csv('/table.csv')
data.head()

"""## 2.2 Working with Data

The columns in the dataframe are as follows:

- N: Number of full gyroscope rotations in the experiment;
- t, in seconds: Time of the experiment;
- σ_t: Measurement error in time;
- mass: Mass of the weight attached to the arm on the gyroscope;
- length: Length of the previously mentioned arm;
- phi: Angle in radians by which the arm dropped during the experiment. This will help us estimate the effect of friction in the gyroscope on precession.

Since physicists like to work with properly dimensioned quantities, convert the mass columns to kilograms and length to meters. Then rename all columns to exclude references to units—use only the names of physical quantities.
"""

data.rename(columns={
    't, sec': 'time',
    'sigma_t, sec': 'sigma_time',
    'mass, gramm': 'mass',
    'length, cm': 'length',
    'phi, rad': 'phi'
}, inplace=True)

if 'Unnamed:0' in data.columns:
    data.drop(columns=['Unnamed: 0'], inplace=True)
if 'mass'in data:
  data['mass'] = data['mass']/1000.0
if 'lenght' in data:
  data['length'] = data['length']/100.0

data.head()

"""Add new columns to the dataframe with the corresponding names and values, calculated using these formulas:

`omega`: $\Omega = 2 \pi \cdot \frac{N}{t}$

`sigma_omega`: $\sigma_{\Omega} = \Omega / t \cdot \sigma_t$

`omega_down`: $\Omega_{down} = \varphi / t$

`sigma_down`: $\Omega_{down} \cdot \sigma_t / t$

`momentum`: $M = m \cdot g \cdot l$ (`g = 9.8 м/с^2`)

`momentum_down`: $M_{down} = m \cdot \frac{\varphi}{t^2} \cdot l^2$

`sigma_momentum`: $\sigma_{M} = M_{down} \cdot 2 \cdot \frac{\sigma_t}{t}$

"""

# YOUR CODE HERE
data['omega'] = (2*np.pi)*data['time']
data['sigma_omega'] = (data['omega']/data['time'])*data['sigma_down']
data['omega_down'] = data['phi']/data['time']
data['sigma_down'] = data['omega_down'] *(data['sigma_time']/data['time'])
data['momentum'] = data['mass']*9.8*data['length']
data['momentum_down'] = data['mass']*(data['phi']/(data['time'])**2)*(data['length'])**2
data['sigma_momentum'] = data['sigma_down']*2*(data['sigma_time']/data['time'])



data.head()
data.head()

"""You may have already wondered why the experiments with the same mass are repeated so many times. To achieve more stable results, of course! Now calculate the average values of the columns `omega`, `σ_omega`, `momentum`, and `momentum_down` for each unique mass.


**Hint:** The groupby function will help you here. No loops allowed!
"""

# YOUR CODE HERE
grouped_data = data.groupby('mass')[['omega','sigma_omega','momentum','momentum_down']].mean()


grouped_data

"""## 2.3 Simple Graphs and Least Squares Method (LSM)

It's now time to reintroduce the Least Squares Method (LSM). Of course, we won't make you write LSM yourself! <s>We're not that cruel</s>

In NumPy, the function [np.polyfit](https://numpy.org/devdocs/reference/generated/numpy.polyfit.html) calculates a polynomial of a given degree that best fits `y(x)` using LSM for the provided `x`, `y`, and degree `p`.

The function [np.polyval](https://numpy.org/devdocs/reference/generated/numpy.polyval.html) evaluates the polynomial `P(x)` with given coefficients.

Your task is to plot the dependence $\Omega (M)$ (angular velocity on moment of inertia). The graph should include experimental points and a line fitted using the least squares method. In the legend, include the polynomial with the calculated coefficients. Don't forget to label the axes (font size 14), add a grid, and provide a suitable title (font size 18)!

<center><img src='https://drive.google.com/uc?export=view&id=1xumON0195iA4HGSqvpS0FAhPGxuCdKH8' width=600>

Example of the resulting graph</center>
"""

omega_np = np.array(grouped_data.omega)
momentum_np = np.array(grouped_data.momentum)

# Take advantage of np.polyfit
# coefs = ...

# To make the line extend smoothly beyond the points
x_lsq = np.linspace(momentum_np.min() * 0.5, momentum_np.max() * 1.1, 100)

# Apply np.polyval to the coefficients and x_lsq
# y_lsq = ...

fig = plt.figure(figsize=(12, 8))

# YOUR CODE HERE
# ...
plt.show()

"""`np.polyfit` also estimates errors! Specifically, it returns the covariance matrix for the least squares method. Without delving into the math, all you need to know is that the diagonal elements are the variances of the calculated coefficients. To get the actual error $\sigma$, take the square root of these variances.

Also, note the `W` parameter, which sets point weights for the estimate. If $y_{error}$ values are known, you can set weights as $W = 1 / y_{error}$ to get an even more precise line. To account for $x$ errors as well, you'd need other methods (but $y$ errors alone are likely sufficient).

Suppose there's been a mishap, and the errors have increased tenfold!
"""

grouped_data['sigma_down'] *= 10
grouped_data['sigma_momentum'] *= 10

"""Now, you need to plot the dependence $\Omega_{down} (M_{down})$ <b>(not $\Omega(M)$!)</b> for points with error bars. In addition to plotting the line from the least squares estimate, include the error estimates for the coefficients. So, plot three lines: $k \cdot x + b$ from the LSM, $(k - \sigma_k) \cdot x + (b - \sigma_b)$, and $(k + \sigma_k) \cdot x + (b + \sigma_b)$, filling the area between these lines with shading (use `plt.fill_between` for this). Keep the rest of the styling as in the previous task.

_Note: Often in LSM, only the error for `k` is considered, leaving out `σ_b`._

<center><img src='https://drive.google.com/uc?export=view&id=1SriaMzJah7F610ocIK_O1-HqqtMQgxlg' width=600>

Example of the resulting graph</center>
"""

omega_down_np = np.array(grouped_data.omega_down)
momentum_down_np = np.array(grouped_data.momentum_down)

# Familiar polyfit, but with an additional parameter and returning covariance!
# coefs, cov = ...

# To make the line plot smoothly again
x_lsq = np.linspace(momentum_down_np.min() * 0.3, momentum_down_np.max() * 1.1, 100)

# Calculate the square root of the diagonal elements; you should get an array of size (2,)
# lsq_stds = ...

# Familiar polyfit, but done three times
# y_lsq = ...
# y_lsq_lower = ...
# y_lsq_upper = ...

fig = plt.figure(figsize=(12, 8))

# YOUR CODE HERE
# ...
plt.show()

"""# 3. Working with the Dataset (3 points)

The Iris dataset was used in R.A. Fisher's 1936 paper “The Use of Multiple Measurements in Taxonomic Problems” and is now commonly used by beginner data analysts.

The dataset includes three species of iris flowers, with 50 samples for each species, along with several properties of each flower. One species is linearly separable from the other two, but the latter two are not linearly separable from each other.

The columns in this dataset:

Identifier (Id) \\
Sepal length in cm (SepalLengthCm) \\
Sepal width in cm (SepalWidthCm) \\
Petal length in cm (PetalLengthCm) \\
Petal width in cm (PetalWidthCm) \\
Species (Species) \\

<font color='red'>Attention!</font> All plots in this part should be labeled!
"""

sns.set_style("darkgrid")

# Read the CSV file into a pandas DataFrame using pd.read_csv
iris = # TODO
iris.head()

# The Id column isn’t very useful,
# so let’s remove it- HINT: use the `drop` method.

# TODO
iris.head()

"""Let's check how many different iris species we have—there should be three, with 50 samples each. Use `value_counts` to see the possible values in the species column."""

# TODO

"""## 3.1 Petal Length and Width Plots

Let's examine whether petal width and length are related—use `sns.scatterplot` to display the OXY values. Remember to label the plot and the axes!
"""

# TODO

"""What conclusions can you draw from the scatter plot? What is missing from the plot to make it more informative?

<font color='red'>YOUR ANSWER HERE</font>

Let's try other plot types for the same purpose from the seaborn library—`sns.jointplot`, and also try to color the points based on iris species using `sns.facetgrid`. Display these two plots in the following cells and analyze them.
"""

#TODO



"""What conclusions can you draw from these plots? Which of the three methods — scatterplot, jointplot, or facetgrid seems the best to you?

<font color='red'> YOUR ANSWER HERE </font>

## 3.2 Distribution Plots of Petal Length Values

Plot a “box-and-whisker plot” with `sns.boxplot` and its counterpart with `sns.violinplot`. The x-axis should represent the iris species, and the y-axis should represent petal length.
"""

# TODO



"""Compare the two types of plots. Which one is more informative, and which is more visually appealing in your opinion? What information can we gather from these plots?

<font color='red'> YOUR ANSWER HERE </font>

## 3.3 Pairwise Feature Comparison Plots


Let's create a 4x4 grid of plots where all possible pairs of features (petal length/width, sepal length/width) are displayed. Use `sns.pairplot`, and don't forget to specify the `hue` parameter.
"""

# TODO

"""What information can you gather about the feature relationships from this plot?

<font color='red'> YOUR ANSWER HERE </font>

What plots are on the diagonal of this grid?

<font color='red'> YOUR ANSWER HERE </font>

Try replacing the diagonal plots with potentially more informative ones (hint: `sns.pairplot` has a special parameter for this in the documentation). Display the resulting plot.
"""

# TODO

"""What plots are now on the diagonal? Does it seem more informative now?

<font color='red'> YOUR ANSWER HERE </font>
"""